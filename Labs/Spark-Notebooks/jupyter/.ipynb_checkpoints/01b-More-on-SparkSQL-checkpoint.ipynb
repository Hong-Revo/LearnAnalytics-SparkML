{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession, DataFrames and Datasets\n",
    "[_Ali Zaidi_](mailto:alizaidi@microsoft.com)\n",
    "\n",
    "This notebook contains an overview of Spark SQL and related APIs. Links to further documentation are provided when we introduce new terms and topics, and are recommended readings/viewings for those looker to go deeper than the basics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Spark SQL\n",
    "\n",
    "[Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html) is the standard Spark core module for processing structured data. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL enforces a richer structure on top of the data being processed, thereby giving the computation framework the ability to optimize. \n",
    "\n",
    "Internally, Spark SQL utilizes the structure and metadata information to perform rich optimizations through [Catalyst](https://spark-summit.org/2016/events/deep-dive-into-catalyst-apache-spark-20s-optimizer/) and [Tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)). There are a variety of ways to interact with Spark SQL, but the standard entry point for data scientists and analysts will be through Spark SQL and the Dataset/DataFrame API. Furthermore, Spark SQL always utilizes the same execution engine to process a query, regardless of which API or language you are using to express the query. This provides performance parity across the various APIs, and also developers to switch easily across APIs to express their analysis in the simplest and most logical format possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dude, where's my session?\n",
    "\n",
    "While the flexibility to choose from different APIs and languages is empowering for developers with skills and experience in particular domains, it can be confusing for those starting off. Let's quickly review the SparkSession variable that's in Spark 2.x to clarify and delete any possible confusion.\n",
    "\n",
    "\n",
    "### Spark Session\n",
    "\n",
    "In Spark 1.x, there were two different entry points to [Spark SQL](http://spark.apache.org/docs/1.6.3/sql-programming-guide.html): the SQLContext and the HiveContext. As you might be able to infer, the HiveContext provided an interface to Hive tables and the metastore, but also provided some optimizations over the simpler SparkContext.  \n",
    "\n",
    "This dual-entry mechanism made it a bit confusing for those just trying to create DataFrames and run queries.\n",
    "\n",
    "In Spark 2.0, Spark introduced the more universal environment called [SparkSession](https://docs.databricks.com/spark/latest/gentle-introduction/sparksession.html), a new entry point that subsumes SQLContext and HiveContext. For backward compatibility, we still have the Hive and SQL Contexts available as part of the Spark environment if you want to reuse old scripts.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our HDInsight Spark 2.0 cluster, a Jupyter Spark session will automatically define a SparkSession when you submit your first statement under the variable name `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SparkSession environment is a high-level API in Spark. Alternatively, you can actually think of it as a DSL (domain-specific language) for structured data processing on Spark. What this means, is that SparkSession adn Spark SQL are constructs on top of the lower engine of Spark, designed to make human-readable queries and compile them into more performant, highly optimized operations that run as close to the Spark internals as possible.\n",
    "If you'd like to see the underlying skeleton that our SparkSession is running on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be useful for creating lower primitive RDDs or changing configuration parameters, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with metadata directly\n",
    "\n",
    "SparkSession also includes a \"catalog\" method that contains methods to work with the metastore (i.e. data catalog). Methods there return Datasets so you can use the same Dataset API to play with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// To get a list of tables in the current database\n",
    "val tables = spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute SQL queries over the data, use `spark.sql`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val pr_head = spark.sql(\"select * from pullrequest limit 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or directly with Jupyter magic commands (specified through `%%sql`) at the top of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from pullrequest limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  },
  "widgets": {
   "state": {
    "cb12680d4efb4d3a99a7c9e410f3bc30": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
