{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data stored in Azure Data Lake Store\n",
    "\n",
    "Azure Data Lake Store can be used as the storage account associated with a HDInsight Spark cluster. An HDInsight cluster can have a default storage and additional storage. The URL to access the cluster storage using ADLS is:\n",
    "\n",
    "    adl:///<adls-name>.azuredatalakestore.net/clusters/<cluster-name>\n",
    "    \n",
    "The URL to access only the default storage is:\n",
    "\n",
    "    adl:///<path>\n",
    "\n",
    "This notebook provides examples of how to read data from WASB into a Spark context and then perform operations on that data. The notebook also provides examples of how to write the output of Spark jobs directly into a WASB location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Read data from ADLS into Spark\n",
    "\n",
    "The examples below read from the default storage account associated with your Spark cluster so the URL used in the examples is `adl:///<path>`. However, you can also read from an additional storage account (e.g., wasb) with the following syntax:\n",
    "\n",
    "    wasb[s]://<containername>@<accountname>.blob.core.windows.net/<path>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Notebook setup\n",
    "\n",
    "When using PySpark kernel notebooks on HDInsight, there is no need to create a SparkContext or a SparkSession; a SparkSession which has the SparkContext is created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\n",
    "- SparkSession (spark)\n",
    "\n",
    "To run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an RDD of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1485637644503_0313</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-sparkt.cmrfjkkbzhnerls1gesmmcccaa.cx.internal.cloudapp.net:8088/proxy/application_1485637644503_0313/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.7:30060/node/containerlogs/container_1485637644503_0313_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# textLines is an RDD of strings\n",
    "textLines = spark.sparkContext.textFile('adl:///example/data/gutenberg/ulysses.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an RDD of key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# seqFile is an RDD of key-value pairs\n",
    "seqFile = spark.sparkContext.sequenceFile('adl:///example/data/people.seq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe from parquet files\n",
    "\n",
    "Create a dataframe from an input parquet file. For more information about parquet files, see [here](http://spark.apache.org/docs/2.0.0/sql-programming-guide.html#parquet-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parquetFile is a dataframe that matches the schema of the input parquet file\n",
    "parquetFile = spark.read.parquet('adl:///example/data/people.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe from JSON document\n",
    "\n",
    "Create a dataframe that matches the schema of the input JSON document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# jsonFile is a dataframe that matches the schema of the input JSON file\n",
    "jsonFile = spark.read.json('adl:///example/data/people.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an dataframe from CSV files\n",
    "\n",
    "Create a dataframe from a CSV file with headers. Spark can automatically infer its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# csvFile is an dataframe that matches the schema of the input CSV file\n",
    "csvFile = spark.read.csv('adl:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Write data from Spark to ADLS in different formats\n",
    "\n",
    "The examples below show you how to write output data from Spark directly into the storage accounts associated with your Spark cluster. If you are writing to the default storage account, you can provide the output path like this:\n",
    "\n",
    "    adl:///<path>\n",
    "\n",
    "If you are writing to additional storage accounts associated with the cluster, you must provide the output path like this:\n",
    "\n",
    "    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save an RDD as text files\n",
    "\n",
    "If you have an RDD, you can convert it to a text file like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# textLines is an RDD converted into a text file\n",
    "textLines.saveAsTextFile('adl:///example/data/gutenberg/ulysses2py.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a dataframe as text files\n",
    "\n",
    "If you have a dataframe that you want to save as a text file, you must first convert it to an RDD and then save that RDD as a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parquetRDD = parquetFile.rdd\n",
    "parquetRDD.saveAsTextFile('adl:///example/data/peoplepy.txt')\n",
    "# parquetRDD is a dataframe converted into RDD. parquetRDD can then be converted into a text file using RDD methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a dataframe as Parquet, JSON or CSV\n",
    "\n",
    "If you have a dataframe, you can save it to Parquet or JSON with the `.write.parquet()`, `.write.json()` and `.write.csv()` methods respectively.\n",
    "\n",
    "Dataframes can be saved in any format, regardless of the input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parquetFile.write.json('adl:///example/data/people3py.json')\n",
    "csvFile.write.parquet('adl:///example/data/people3py.parquet')\n",
    "jsonFile.write.csv('adl:///example/data/people3py.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an RDD and want to save it as a parquet file or JSON file, you'll have to \n",
    "convert it to a dataframe. See [Interoperating with RDDs](http://spark.apache.org/docs/2.0.0/sql-programming-guide.html#interoperating-with-rdds) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save an RDD of key-value pairs as a sequence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If your RDD isn't made up of key-value pairs then you'll get a runtime error\n",
    "seqFile.saveAsSequenceFile('adl:///example/data/people2py.seq')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
