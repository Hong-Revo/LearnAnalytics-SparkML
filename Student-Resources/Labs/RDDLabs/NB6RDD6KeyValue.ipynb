{"nbformat_minor": 0, "cells": [{"source": "# Working with key/value pair RDDs", "cell_type": "markdown", "metadata": {}}, {"source": "Spark provides specific functions to deal with RDDs which elements are key/value pairs. They are usually used to perform aggregations and other processings by key.  ", "cell_type": "markdown", "metadata": {}}, {"source": "In this notebook we will show how, by working with key/value pairs, we can process our network interactions dataset in a more practical and powerful way than that used in previous notebooks. Key/value pair aggregations will show to be particularly effective when trying to explore each type of tag in our network attacks, in an individual way.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from __future__ import print_function\nimport sys\nif sys.version[0] == 3:\n    xrange = range", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Getting the data and creating the RDD", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data_file = \"/KDD/kddcup.data_10_percent.gz\"\nraw_data = sc.textFile(data_file)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Creating a pair RDD for interaction types", "cell_type": "markdown", "metadata": {}}, {"source": "In this notebook we want to do some exploratory data analysis on our network interactions dataset. More concretely we want to profile each network interaction type in terms of some of its variables such as duration. In order to do so, we first need to create the RDD suitable for that, where each interaction is parsed as a CSV row representing the value, and is put together with its corresponding tag as a key.  ", "cell_type": "markdown", "metadata": {}}, {"source": "Normally we create key/value pair RDDs by applying a function using `map` to the original data. This function returns the corresponding pair for a given RDD element. We can proceed as follows.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "csv_data = raw_data.map(lambda x: x.split(\",\"))\nkey_value_data = csv_data.map(lambda x: (x[41], x)) # x[41] contains the network interaction tag", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We have now our key/value pair data ready to be used. Let's get the first element in order to see how it looks like.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "key_value_data.take(1)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Data aggregations with key/value pair RDDs", "cell_type": "markdown", "metadata": {}}, {"source": "We can use all the transformations and actions available for normal RDDs with key/value pair RDDs. We just need to make the functions work with pair elements. Additionally, Spark provides specific functions to work with RDDs containing pair elements. They are very similar to those available for general RDDs.  ", "cell_type": "markdown", "metadata": {}}, {"source": "For example, we have a `reduceByKey` transformation that we can use as follows to calculate the total duration of each network interaction type.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0]))) \ndurations_by_key = key_value_duration.reduceByKey(lambda x, y: x + y)\n\ndurations_by_key.collect()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We have a specific counting action for key/value pairs.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "counts_by_key = key_value_data.countByKey()\ncounts_by_key", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Using `combineByKey`", "cell_type": "markdown", "metadata": {}}, {"source": "This is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it. We can think about it as the `aggregate` equivalent since it allows the user to return values that are not the same type as our input data.", "cell_type": "markdown", "metadata": {}}, {"source": "For example, we can use it to calculate per-type average durations as follows.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sum_counts = key_value_duration.combineByKey(\n    (lambda x: (x, 1)), # the initial value, with value x and count 1\n    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n)\n\nsum_counts.collectAsMap()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We can see that the arguments are pretty similar to those passed to `aggregate` in the previous notebook. The result associated to each type is in the form of a pair. If we want to actually get the averages, we need to do the division before collecting the results.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "duration_means_by_type = sum_counts.map(lambda (key,value): (key, round(value[0]/value[1],3))).collectAsMap()\n\n# Print them sorted\nfor tag in sorted(duration_means_by_type, key=duration_means_by_type.get, reverse=True):\n    print(tag, duration_means_by_type[tag])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "A small step into understanding what makes a network interaction be considered an attack.", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}