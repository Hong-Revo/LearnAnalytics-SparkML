{"nbformat_minor": 0, "cells": [{"source": "# RDD creation", "cell_type": "markdown", "metadata": {}}, {"source": "In this notebook we will introduce two different ways of getting data into the basic Spark data structure, the **Resilient Distributed Dataset** or **RDD**. An RDD is a distributed collection of elements. All work in Spark is expressed as either creating new RDDs, transforming existing RDDs, or calling actions on RDDs to compute a result. Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.", "cell_type": "markdown", "metadata": {}}, {"source": "#### References", "cell_type": "markdown", "metadata": {}}, {"source": "The reference book for these and other Spark related topics is *Learning Spark* by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia.  ", "cell_type": "markdown", "metadata": {}}, {"source": "The KDD Cup 1999 competition dataset is described in detail [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99).", "cell_type": "markdown", "metadata": {}}, {"source": "## Python Configurations", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from __future__ import print_function\nimport sys\nif sys.version[0] == 3:\n    xrange = range", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Checking Spark Version and Configuration\nOnce you run the chunk above, the notebook will create a few Spark configuration variables for you. The primary variable is the SparkSession variable defined as `spark`.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "print(spark.version)\nprint(sc.version)\nprint(sc.appName)\nprint(sc._conf.toDebugString())", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Getting the data files  ", "cell_type": "markdown", "metadata": {}}, {"source": "In this notebook we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a *Gzip* file that we will download locally.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%local\nimport urllib\nf = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now that we have a local copy of the data, we can copy it over to HDFS.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n!ls\n!hdfs dfs -mkdir /KDD/\n!hdfs dfs -copyFromLocal ./kddcup.data_10_percent.gz /KDD/\n!hdfs dfs -ls /KDD", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Creating a RDD from a file  ", "cell_type": "markdown", "metadata": {}}, {"source": "The most common way of creating an RDD is to load it from a file. Notice that Spark's `textFile` can handle compressed files directly.    ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data_file = \"/KDD/kddcup.data_10_percent.gz\"\nraw_data = sc.textFile(data_file)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now we have our data file loaded into the `raw_data` RDD.", "cell_type": "markdown", "metadata": {}}, {"source": "Without getting into Spark *transformations* and *actions*, the most basic thing we can do to check that we got our RDD contents right is to `count()` the number of lines loaded from the file into the RDD.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "raw_data.count()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We can also check the first few entries in our data.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "raw_data.take(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "In the following notebooks, we will use this raw data to learn about the different Spark transformations and actions.  ", "cell_type": "markdown", "metadata": {}}, {"source": "## Creating and RDD using `parallelize`", "cell_type": "markdown", "metadata": {}}, {"source": "Another way of creating an RDD is to parallelize an already existing list.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "a = range(100)\n\ndata = sc.parallelize(a)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "As we did before, we can `count()` the number of elements in the RDD.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data.count()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "As before, we can access the first few elements on our RDD.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data.take(5)", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}