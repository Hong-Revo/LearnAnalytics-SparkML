{"nbformat_minor": 1, "cells": [{"execution_count": 1, "cell_type": "code", "source": "from pyspark.ml.feature import StringIndexer, StandardScaler, IndexToString\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.ml.tuning import ParamGridBuilder\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import UserDefinedFunction, stddev\nfrom pyspark.sql.functions import * # array, desc, asc, mean\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, IntegerType\n# %matplotlib inline\n\n# sqlc = SQLContext(sc)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>16</td><td>application_1497292409819_0021</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-azdont.oktssmvuh14exkk2kow5enazuc.gx.internal.cloudapp.net:8088/proxy/application_1497292409819_0021/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.10:30060/node/containerlogs/container_e02_1497292409819_0021_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "import numpy as np\nimport json\nimport pickle\nimport shutil\nimport os\n# import seaborn as sns\nfrom time import mktime\nfrom datetime import datetime", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 14, "cell_type": "code", "source": "class YelpRecommender(object):\n    def __init__(self, folder='/', business_filename=None, reviews_filename=None):\n        self._folder = folder\n        if business_filename is None:\n            business_filename = os.path.join(folder, 'yelp_academic_dataset_business.json')\n        if reviews_filename is None:\n            reviews_filename = os.path.join(folder, 'yelp_academic_dataset_review.json')\n        \n        self._city = None\n        self._category = None\n        self._min_per_user = 0\n        self._scaler = {'mean': True, 'stdev': False}\n        \n        self._verbose = True\n        self._business = None\n        self._reviews = None\n        self._city_business = None\n        self._city_reviews = None\n        self._dataset = None\n        self._training = None\n        self._test = None\n        self._best_model = None\n        self._best_parms = None\n        self._eval = None\n        \n        self._business_dict = {}\n        self._user_dict = {}\n        self._review_dict = {}\n        \n        self.load_business(business_filename)\n        self.load_reviews(reviews_filename)\n    \n    def console(self, message):\n        if self._verbose:\n            print(message)\n            \n    @property\n    def best_model(self):\n        return self._best_model\n    \n    @property\n    def best_parms(self):\n        return self._best_parms\n    \n    @property\n    def evaluation(self):\n        return self._eval\n    \n    @property\n    def all_users(self):\n        return self._user_dict.keys()\n    \n    @staticmethod\n    def rest_to_json(row):\n        return {row.business_idn: {'name': row.name,\n                                   'address': row.address, \n                                   'stars': row.stars, \n                                   'categories': row.categories}}\n    \n    def load_business(self, filename):\n        self.console('Loading businesses data...')\n        if os.path.exists('./metastore_db/dbex.lck'):\n            os.remove('./metastore_db/dbex.lck')    \n        business = spark.read.json(filename)\n        business = business.select('business_id', 'name', 'city', 'stars', 'categories', 'address')\n        self._business = business\n\n    def load_reviews(self, filename):\n        self.console('Loading reviews data...')\n        if os.path.exists('./metastore_db/dbex.lck'):\n            os.remove('./metastore_db/dbex.lck')    \n        reviews = spark.read.json(filename)\n        reviews = reviews.select('user_id', 'business_id', 'stars', 'date')\n        self._reviews = reviews\n\n    def _get_city_businesses(self, category):\n        def is_category_listed(name, categories):\n            listed = 0.0\n            if categories is not None:\n                if name in categories:\n                    listed = 1.0\n            return listed\n        \n        is_category = UserDefinedFunction(lambda c: is_category_listed(category, c), DoubleType())\n\n        business_cat = self._business.withColumn('is_category', is_category('categories')) \\\n                                    .filter('is_category = 1.0') \\\n                                    .drop('is_category')\n\n        city_business = business_cat.filter('city = \"' + self._city +'\"')\n\n        business_indexer = StringIndexer().setInputCol('business_id').setOutputCol('business_idn')\n        business_idx_model = business_indexer.fit(city_business)\n        city_business = business_idx_model.transform(city_business)\n\n        city_business.cache()\n\n        rest = city_business.select('business_idn', 'name', 'address', 'stars', 'categories')\n#         rest = map(self.rest_to_json, rest)\n#         rest = {k: v for d in rest for k, v in d.items()}\n\n        filename = os.path.join(folder, '%s_%s_business_' % (self._city.replace(' ', '_'), self._category))\n        self.console('Saving business data to %s ...' % filename)\n#         with open(filename, 'wb') as f:\n#             pickle.dump(rest, f)\n        self._business_dict = rest\n        \n        rest.write.parquet(filename)\n\n        self._city_business = city_business\n\n    def _get_city_reviews(self):\n        reviews_city = self._reviews.join(self._city_business.select('business_id', 'business_idn'), on='business_id')\n\n        np.random.seed(42)\n        to_timestamp = UserDefinedFunction(lambda d: mktime(datetime.strptime(d, '%Y-%m-%d').timetuple()) + np.random.random(), DoubleType())\n        df_selected = reviews_city.select('user_id', 'business_idn', 'stars', to_timestamp('date').alias('timestamp')).drop('date')\n\n        if self._min_per_user > 0:\n            df_selected.createOrReplaceTempView('ratings')\n            df_selected = spark.sql('select user_id, business_idn, stars, timestamp \\\n                                    from ratings \\\n                                    where user_id in (select user_id \\\n                                                      from ratings \\\n                                                      group by user_id \\\n                                                      having count(1) > ' + str(self._min_per_user) + ')')\n\n        df_selected.cache()\n        user_indexer = StringIndexer().setInputCol('user_id').setOutputCol('user_idn')\n        user_idx_model = user_indexer.fit(df_selected)\n        df_selected = user_idx_model.transform(df_selected)    \n\n        df_selected.cache()\n        user_last = df_selected.select('user_id', 'timestamp').groupby('user_id').max().rdd.collectAsMap()\n        user_last_lookup = sc.broadcast(user_last)    \n\n        get_last = UserDefinedFunction(lambda c: user_last_lookup.value.get(c), DoubleType())\n        df_selected = df_selected.withColumn('last_time', get_last('user_id'))\n        df_selected.cache()\n\n        user_mapping = df_selected.select('user_id', 'user_idn').groupby('user_id').max().rdd.collectAsMap()\n\n        filename = os.path.join(folder, '%s_%s_user' % (self._city.replace(' ', '_'), self._category))\n        self.console('Saving user data to %s ...' % filename)\n        with open(filename, 'wb') as f:\n            pickle.dump(user_mapping, f)\n        \n#         user_mapping.write.parquet(filename)\n        \n        self._user_dict = user_mapping\n\n        all_visited = df_selected.select('user_idn', 'business_idn').groupby('user_idn').agg(collect_list('business_idn')).rdd.collectAsMap()\n        filename = os.path.join(folder, '%s_%s_review' % (self._city.replace(' ', '_'), self._category))\n        self.console('Saving review data to %s ...' % filename)\n        with open(filename, 'wb') as f:\n            pickle.dump(all_visited, f)\n#         all_visited.write.parquet(filename)\n        self._review_dict = all_visited\n\n        self._city_reviews = df_selected\n\n    def _apply_scaler(self, mean=True, stdev=False):\n        if mean:\n            user_means = self._city_reviews.select('user_id', 'stars').groupby('user_id').mean().rdd.collectAsMap()\n        else:\n            user_means = {}\n\n        user_means_lookup = sc.broadcast(user_means)\n\n        if stdev:\n            user_stds = self._city_reviews.select('user_id', 'stars').groupby('user_id').agg(stddev('stars').alias('stddev')).rdd.collect()\n            user_stds = dict(map(lambda row: (row.user_id, row.stddev if row.stddev > 0.0 else 1.0), user_stds))\n        else:\n            user_stds = {}\n        user_stds_lookup = sc.broadcast(user_stds)    \n\n        center_user_stars = UserDefinedFunction(lambda cols: (float(cols[1]) - user_means_lookup.value.get(cols[0], 0.0))/user_stds_lookup.value.get(cols[0], 1.0), DoubleType())\n\n        df_scaled = self._city_reviews.withColumn('centered', center_user_stars(array('user_id','stars'))) \\\n                        .drop('stars') \\\n                        .withColumnRenamed('centered', 'stars')\n\n        df_scaled.cache()\n\n        self._dataset = df_scaled\n\n    def _training_test_split(self):\n        training = self._dataset.filter('last_time <> timestamp')\n        test = self._dataset.filter('last_time = timestamp')\n        \n        # https://github.com/apache/spark/pull/12896\n        available_business = sc.broadcast(set(training.select('business_idn').distinct().rdd.map(lambda t: t.business_idn).collect()))\n        is_available = UserDefinedFunction(lambda idn: 1.0 if idn in available_business.value else 0.0, DoubleType())    \n        test = test.withColumn('available', is_available('business_idn'))\n        test = test.filter('available = 1.0')\n\n        self._training, self._test = training, test\n\n    def _grid_search(self, paramGrid):\n        model_list = []\n        eval_lists = {'train_rmse': [], 'test_rmse': []}\n\n        param_list = [dict(map(lambda t: (t[0].name, t[1]), params.items())) for params in paramGrid]\n\n        for idx, params in enumerate(param_list):\n            self.console(\"Training model %d/%d with %s\" % (idx + 1, len(param_list), params))\n            model = ALS(userCol=\"user_idn\", itemCol=\"business_idn\", ratingCol=\"stars\", seed=42).setParams(**params)\n            model = model.fit(self._training)\n            model_list.append(model)\n\n            predictions = model.transform(self._training)\n            evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"stars\",\n                                            predictionCol=\"prediction\")\n            train_rmse = evaluator.evaluate(predictions)\n            eval_lists['train_rmse'].append(train_rmse)\n            self.console(\"Train Root-mean-square error = \" + str(train_rmse))\n\n            predictions = model.transform(self._test)\n            evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"stars\",\n                                            predictionCol=\"prediction\")\n            test_rmse = evaluator.evaluate(predictions)\n            eval_lists['test_rmse'].append(test_rmse)\n            self.console(\"Test Root-mean-square error = \" + str(test_rmse))\n\n        best_seq = np.argmin(eval_lists['test_rmse'])\n        best_model = model_list[best_seq]\n        best_parms = param_list[best_seq]\n        \n        self._best_model, self._best_parms, self._eval = best_model, best_parms, eval_lists\n        \n        model_filename = os.path.join(folder, '%s_als_model_regParm%0.1f_rank%d_maxIter%d' % (self._city.replace(' ', '_'), best_parms['regParam'], best_parms['rank'], best_parms['maxIter']))\n        self.console(\"Best model parameters: %s\" % best_parms)\n        self.console(\"Saving best model to %s\" % model_filename)\n        if os.path.exists(model_filename):\n            shutil.rmtree(model_filename)\n        best_model.save(model_filename)\n    \n    def fit(self, city, category, parms, verbose=True):\n        self._verbose = verbose\n        \n        self._city = city\n        self._category = category\n        self._min_per_user = parms['min_per_user']\n        self._scaler = parms['scaler']\n        \n        self.console('Filtering businesses for the city...')\n        self._get_city_businesses(self._category)\n        self.console('Filtering reviews for the city...')\n        self._get_city_reviews()\n        self.console('Scaling user ratings...')\n        self._apply_scaler(mean=self._scaler['mean'], stdev=self._scaler['stdev'])\n        self.console('Splitting dataset into training and test...')\n        self._training_test_split()\n\n        als = ALS()\n        paramGrid = ParamGridBuilder() \\\n            .addGrid(als.rank, parms['als']['rank']) \\\n            .addGrid(als.maxIter, parms['als']['maxIter']) \\\n            .addGrid(als.regParam, parms['als']['regParam']) \\\n            .build()\n\n        self.console('Performing grid search over parameters...')\n        self._grid_search(paramGrid)\n        self.console('Finished!')\n        \n    def recommend(self, user_id, n):\n        n_business = len(self._business_dict)\n        user_idn = self._user_dict[user_id]\n        visited = self._review_dict[user_idn]\n        test_user = spark.createDataFrame([Row(user_idn=user_idn, business_idn=float(i)) for i in list(set(range(n_business)).difference(set(visited)))])\n\n        pred_test = self._best_model.transform(test_user).na.fill(-5.0)\n        top_pred = pred_test.orderBy(desc('prediction')).select('business_idn').rdd.map(lambda row: row.business_idn).take(n)\n        response = map(lambda idn: self._business_dict[idn], top_pred)\n        return json.dumps(response)\n    \n    def list_ratings(self, user_id, n):\n        user_idn = self._user_dict[user_id]\n        visited = self._review_dict[user_idn]\n        response = sorted(map(lambda idn: self._business_dict[idn], visited), key=lambda k: k['stars'], reverse=True)[:n]\n        return json.dumps(response)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 15, "cell_type": "code", "source": "folder = '/yelp/data/'\n\nrec = YelpRecommender(folder)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Loading businesses data...\nLoading reviews data..."}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "rec", "outputs": [{"output_type": "stream", "name": "stdout", "text": "<YelpRecommender object at 0x7f6fbb58a518>"}], "metadata": {"collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "parms = {'min_per_user': 4,\n         'scaler': {'mean': True, 'stdev': False},\n         'als': {'rank': [5, 10, 20],\n                 'maxIter': [5, 10],\n                 'regParam': [1.0, 0.3, 0.1]}}\n\nrec.fit('Toronto', 'Restaurants', parms)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "[Errno 2] No such file or directory: '/yelp/data/Toronto_Restaurants_user'\nTraceback (most recent call last):\n  File \"<stdin>\", line 249, in fit\n  File \"<stdin>\", line 145, in _get_city_reviews\nFileNotFoundError: [Errno 2] No such file or directory: '/yelp/data/Toronto_Restaurants_user'\n\n"}], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "pred = rec.best_model.transform(rec._test)\npred_df = pred.toPandas()", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "sns.lmplot(x='stars', y='prediction', data=pred_df)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "user_id = np.random.choice(rec.all_users)\nprint(user_id)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "json.loads(rec.recommend(user_id, 3))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "json.loads(rec.list_ratings(user_id, 3))", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}, "anaconda-cloud": {}}}