{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for this notebook is from the **yelp-data-challenge**: https://www.yelp.com/dataset_challenge/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import core libraries\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import shutil\n",
    "# import seaborn as sns\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "from pyspark.ml.feature import StringIndexer, StandardScaler, IndexToString\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import UserDefinedFunction, stddev\n",
    "from pyspark.sql.functions import * # array, desc, asc, mean\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, IntegerType\n",
    "# %matplotlib inline\n",
    "\n",
    "## set up configuration for both py2 and py3\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "if sys.version[0] == 3:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Spark Version and Configuration\n",
    "\n",
    "Once you run the chunk above, the notebook will create a few Spark configuration variables for you. The primary variable is the SparkSession variable defined as spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(spark.version)\n",
    "print(sc.version)\n",
    "print(sc.appName)\n",
    "# print(sc._conf.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data into Spark DataFrames\n",
    "\n",
    "To begin our analysis, we will read in some data into Spark DataFrames. The data we will be analyzing is the yelp data we have saved in HDFS under `/yelp/yelp-data/`. Let's read in the business dataset first. Here we'll use the built-in json `DataFrameReader` from the `spark` Spark Session object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business = spark.read.json(\"/yelp/data/yelp_academic_dataset_business.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business = business.select('business_id', 'name', 'city', 'stars', 'state',\n",
    "                           'categories', 'address', 'categories', 'review_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries Against DataFrames\n",
    "\n",
    "The `sql` function within a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame. In HDInsight Jupyter notebooks, there are a few additional [parameters](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-spark-jupyter-notebook-kernels#parameters-supported-with-the-sql-magic) you can use to manipulate the output from SQL queries.\n",
    "\n",
    "In order to run queries against a Spark DataFrame, you need to first create a temporary view for Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business.createOrReplaceTempView(\"business\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT state, COUNT(*) as cnt FROM business GROUP BY state ORDER BY cnt DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT city, COUNT(*) as cnt FROM business GROUP BY city ORDER BY cnt DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = spark.read.json(\"/yelp/data/yelp_academic_dataset_review.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews.groupBy(\"stars\").count().explain(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews.groupBy(\"stars\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews.groupBy(\"stars\", \"funny\").count().sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biz_reviews = business.join(reviews, \n",
    "                                business[\"business_id\"] == reviews[\"business_id\"], \n",
    "                                \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biz_reviews.explain(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biz_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biz_reviews.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
