# Create Spark Context

The `sparklyr` package has a handy function for creating a Spark context, `spark_connect`. We can first pull the default spark configurations using the `spark_config` function, and modify it with the optimal parameters for our cluster. An easy way of determining this is first fixing a value of `numExecutors`, i.e., how many executors you need. My rule of thumb is to have 3 - 5 executors per YARN container, and 1 - 3 YARN containers per worker node (this is for medium-sized worker VMs, for worker VMs with > 100 GB of RAM, you may want to double this). 

Note, this configuration set up differs from the method that is used by the `SparkR` package.

```{r spark_context}

library(sparklyr)

## D13_v2 = 56 GBs & 8 cores
## executor memory: 50 / 3 ~ 16
## executor CPU: 8 / 3 ~ 2
## num executors: 2 * num_workers = 8

conf <- spark_config()
conf$spark.executor.cores <- 2
conf$spark.executor.memory <- "16G"
conf$spark.yarn.am.cores  <- 1
conf$spark.yarn.am.memory <- "4G"
conf$spark.dynamicAllocation.enabled <- "false"
conf$spark.executor.instances <- 8

sc <- spark_connect(master = "yarn-client")

```

# Download Sample Data 

```{r download_data}

# download.file("https://alizaidi.blob.core.windows.net/training/sample_taxi.csv", "sample_taxi.csv")
# or the larger data!
download.file("http://alizaidi.blob.core.windows.net/training/taxi_large.csv", "taxi_large.csv")

wasb_taxi <- "/NYCTaxi/sample"
rxHadoopListFiles("/")
rxHadoopMakeDir(wasb_taxi)
rxHadoopCopyFromLocal("taxi_large.csv", wasb_taxi)
rxHadoopCommand("fs -cat /NYCTaxi/sample/taxi_large.csv | head")


```

Let's also download the sample data that we can work with locally.

```{r download_sample}
taxi_url <- "http://alizaidi.blob.core.windows.net/training/trainingData/manhattan_df.rds"
taxi_df  <- readRDS(gzcon(url(taxi_url)))
(taxi_df <- tbl_df(taxi_df))
```


# Import Data

To import data from csv files, we can use the `spark_read_csv` function, which is basically a wrapper for the `read.df` function using the __databricks.spark.csv__ package.

```{r import_csv}

taxi <- spark_read_csv(sc,
                       path = wasb_taxi,
                       "taxisample",
                       header = TRUE)


```

