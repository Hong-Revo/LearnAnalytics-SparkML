# Create Spark Context

The `sparklyr` package has a handy function for creating a Spark context. This differs from the method that is used by the `SparkR` package.

```{r spark_context}

library(sparklyr)
sc <- spark_connect(master = "yarn-client")

```

# Download Sample Data 

```{r download_data}

# download.file("https://alizaidi.blob.core.windows.net/training/sample_taxi.csv", "sample_taxi.csv")
# or the larger data!
download.file("http://alizaidi.blob.core.windows.net/training/taxi_large.csv", "taxi_large.csv")

wasb_taxi <- "/NYCTaxi/sample"
rxHadoopListFiles("/")
rxHadoopMakeDir(wasb_taxi)
rxHadoopCopyFromLocal("taxi_large.csv", wasb_taxi)
rxHadoopCommand("fs -cat /NYCTaxi/sample/taxi_large.csv | head")


```

Let's also download the sample data that we can work with locally.

```{r download_sample}
taxi_url <- "http://alizaidi.blob.core.windows.net/training/trainingData/manhattan_df.rds"
taxi_df  <- readRDS(gzcon(url(taxi_url)))
(taxi_df <- tbl_df(taxi_df))
```


# Import Data

To import data from csv files, we can use the `spark_read_csv` function, which is basically a wrapper for the `read.df` function using the __databricks.spark.csv__ package.

```{r import_csv}

taxi <- spark_read_csv(sc,
                       path = wasb_taxi,
                       "taxisample",
                       header = TRUE)


```

